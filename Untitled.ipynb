{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " from gensim import corpora\n",
    "\n",
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "           \"A survey of user opinion of computer system response time\",\n",
    "              \"The EPS user interface management system\",\n",
    "              \"System and human system engineering testing of EPS\",\n",
    "              \"Relation of user perceived response time to error measurement\",\n",
    "              \"The generation of random binary unordered trees\",\n",
    "              \"The intersection graph of paths in trees\",\n",
    "              \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "              \"Graph minors A survey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Copyright (C) 2010 Radim Rehurek <radimrehurek@seznam.cz>\n",
    "# Licensed under the GNU LGPL v2.1 - http://www.gnu.org/licenses/lgpl.html\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Corpus for the DML-CZ project.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import logging\n",
    "import os.path\n",
    "\n",
    "from gensim import interfaces, matutils\n",
    "import dictionary  # for constructing word->id mappings\n",
    "\n",
    "\n",
    "logger = logging.getLogger('gensim.corpora.dmlcorpus')\n",
    "\n",
    "\n",
    "class DmlConfig(object):\n",
    "    \"\"\"\n",
    "    DmlConfig contains parameters necessary for the abstraction of a 'corpus of\n",
    "    articles' (see the `DmlCorpus` class).\n",
    "\n",
    "    Articles may come from different sources (=different locations on disk/network,\n",
    "    different file formats etc.), so the main purpose of DmlConfig is to keep all\n",
    "    sources in one place.\n",
    "\n",
    "    Apart from glueing sources together, DmlConfig also decides where to store\n",
    "    output files and which articles to accept for the corpus (= an additional filter\n",
    "    over the sources).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, configId, resultDir, acceptLangs=None):\n",
    "        self.resultDir = resultDir  # output files will be stored in this directory\n",
    "        self.configId = configId\n",
    "        self.sources = {}  # all article sources; see sources.DmlSource class for an example of source\n",
    "\n",
    "        if acceptLangs is None:  # which languages to accept\n",
    "            acceptLangs = {'any'}  # if not specified, accept all languages (including unknown/unspecified)\n",
    "        self.acceptLangs = set(acceptLangs)\n",
    "        logger.info('initialized %s', self)\n",
    "\n",
    "    def resultFile(self, fname):\n",
    "        return os.path.join(self.resultDir, self.configId + '_' + fname)\n",
    "\n",
    "    def acceptArticle(self, metadata):\n",
    "        lang = metadata.get('language', 'unk')\n",
    "        if 'any' not in self.acceptLangs and lang not in self.acceptLangs:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def addSource(self, source):\n",
    "        sourceId = str(source)\n",
    "        assert sourceId not in self.sources, \"source %s already present in the config!\" % sourceId\n",
    "        self.sources[sourceId] = source\n",
    "\n",
    "    def __str__(self):\n",
    "        return (\"DmlConfig(id=%s, sources=[%s], acceptLangs=[%s])\" %\n",
    "                (self.configId, ', '.join(self.sources.iterkeys()), ', '.join(self.acceptLangs)))\n",
    "# endclass DmlConfig\n",
    "\n",
    "\n",
    "class DmlCorpus(interfaces.CorpusABC):\n",
    "    \"\"\"\n",
    "    DmlCorpus implements a collection of articles. It is initialized via a DmlConfig\n",
    "    object, which holds information about where to look for the articles and how\n",
    "    to process them.\n",
    "\n",
    "    Apart from being a regular corpus (bag-of-words iterable with a `len()` method),\n",
    "    DmlCorpus has methods for building a dictionary (mapping between words and\n",
    "    their ids).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.documents = []\n",
    "        self.config = None\n",
    "        self.dictionary = dictionary.Dictionary()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.documents)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        The function that defines a corpus -- iterating over the corpus yields\n",
    "        bag-of-words vectors, one for each document.\n",
    "\n",
    "        A bag-of-words vector is simply a list of ``(tokenId, tokenCount)`` 2-tuples.\n",
    "        \"\"\"\n",
    "        for docNo, (sourceId, docUri) in enumerate(self.documents):\n",
    "            source = self.config.sources[sourceId]\n",
    "\n",
    "            contents = source.getContent(docUri)\n",
    "            words = [source.normalizeWord(word) for word in source.tokenize(contents)]\n",
    "            yield self.dictionary.doc2bow(words, allowUpdate=False)\n",
    "\n",
    "    def buildDictionary(self):\n",
    "        \"\"\"\n",
    "        Populate dictionary mapping and statistics.\n",
    "\n",
    "        This is done by sequentially retrieving the article fulltexts, splitting\n",
    "        them into tokens and converting tokens to their ids (creating new ids as\n",
    "        necessary).\n",
    "        \"\"\"\n",
    "        logger.info(\"creating dictionary from %i articles\", len(self.documents))\n",
    "        self.dictionary = dictionary.Dictionary()\n",
    "        numPositions = 0\n",
    "        for docNo, (sourceId, docUri) in enumerate(self.documents):\n",
    "            if docNo % 1000 == 0:\n",
    "                logger.info(\"PROGRESS: at document #%i/%i (%s, %s)\", docNo, len(self.documents), sourceId, docUri)\n",
    "            source = self.config.sources[sourceId]\n",
    "            contents = source.getContent(docUri)\n",
    "            words = [source.normalizeWord(word) for word in source.tokenize(contents)]\n",
    "            numPositions += len(words)\n",
    "\n",
    "            # convert to bag-of-words, but ignore the result -- here we only care about updating token ids\n",
    "            _ = self.dictionary.doc2bow(words, allowUpdate=True)  # noqa:F841\n",
    "        logger.info(\n",
    "            \"built %s from %i documents (total %i corpus positions)\",\n",
    "            self.dictionary, len(self.documents), numPositions\n",
    "        )\n",
    "\n",
    "    def processConfig(self, config, shuffle=False):\n",
    "        \"\"\"\n",
    "        Parse the directories specified in the config, looking for suitable articles.\n",
    "\n",
    "        This updates the self.documents var, which keeps a list of (source id,\n",
    "        article uri) 2-tuples. Each tuple is a unique identifier of one article.\n",
    "\n",
    "        Note that some articles are ignored based on config settings (for example\n",
    "        if the article's language doesn't match any language specified in the\n",
    "        config etc.).\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.documents = []\n",
    "        logger.info(\"processing config %s\", config)\n",
    "        for sourceId, source in config.sources.iteritems():\n",
    "            logger.info(\"processing source '%s'\", sourceId)\n",
    "            accepted = []\n",
    "            for articleUri in source.findArticles():\n",
    "                meta = source.getMeta(articleUri)  # retrieve metadata (= dictionary of key->value)\n",
    "                if config.acceptArticle(meta):  # do additional filtering on articles, based on the article's metadata\n",
    "                    accepted.append((sourceId, articleUri))\n",
    "            logger.info(\"accepted %i articles for source '%s'\", len(accepted), sourceId)\n",
    "            self.documents.extend(accepted)\n",
    "\n",
    "        if not self.documents:\n",
    "            logger.warning('no articles at all found from the config; something went wrong!')\n",
    "\n",
    "        if shuffle:\n",
    "            logger.info(\"shuffling %i documents for random order\", len(self.documents))\n",
    "            import random\n",
    "            random.shuffle(self.documents)\n",
    "\n",
    "        logger.info(\"accepted total of %i articles for %s\", len(self.documents), str(config))\n",
    "\n",
    "    def saveDictionary(self, fname):\n",
    "        logger.info(\"saving dictionary mapping to %s\", fname)\n",
    "        fout = open(fname, 'w')\n",
    "        for tokenId, token in self.dictionary.id2token.iteritems():\n",
    "            fout.write(\"%i\\t%s\\n\" % (tokenId, token))\n",
    "        fout.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def loadDictionary(fname):\n",
    "        result = {}\n",
    "        for lineNo, line in enumerate(open(fname)):\n",
    "            pair = line[:-1].split('\\t')\n",
    "            if len(pair) != 2:\n",
    "                continue\n",
    "            wordId, word = pair\n",
    "            result[int(wordId)] = word\n",
    "        return result\n",
    "\n",
    "    def saveDocuments(self, fname):\n",
    "        logger.info(\"saving documents mapping to %s\", fname)\n",
    "        fout = open(fname, 'w')\n",
    "        for docNo, docId in enumerate(self.documents):\n",
    "            sourceId, docUri = docId\n",
    "            intId, pathId = docUri\n",
    "            fout.write(\"%i\\t%s\\n\" % (docNo, repr(docId)))\n",
    "        fout.close()\n",
    "\n",
    "    def saveAsText(self):\n",
    "        \"\"\"\n",
    "        Store the corpus to disk, in a human-readable text format.\n",
    "\n",
    "        This actually saves multiple files:\n",
    "\n",
    "        1. Pure document-term co-occurence frequency counts, as a Matrix Market file.\n",
    "        2. Token to integer mapping, as a text file.\n",
    "        3. Document to document URI mapping, as a text file.\n",
    "\n",
    "        The exact filesystem paths and filenames are determined from the config.\n",
    "        \"\"\"\n",
    "        self.saveDictionary(self.config.resultFile('wordids.txt'))\n",
    "        self.saveDocuments(self.config.resultFile('docids.txt'))\n",
    "        matutils.MmWriter.writeCorpus(self.config.resultFile('bow.mm'), self)\n",
    "\n",
    "    def articleDir(self, docNo):\n",
    "        \"\"\"\n",
    "        Return absolute normalized path on filesystem to article no. `docNo`.\n",
    "        \"\"\"\n",
    "        sourceId, (_, outPath) = self.documents[docNo]\n",
    "        source = self.config.sources[sourceId]\n",
    "        return os.path.join(source.baseDir, outPath)\n",
    "\n",
    "    def getMeta(self, docNo):\n",
    "        \"\"\"\n",
    "        Return metadata for article no. `docNo`.\n",
    "        \"\"\"\n",
    "        sourceId, uri = self.documents[docNo]\n",
    "        source = self.config.sources[sourceId]\n",
    "        return source.getMeta(uri)\n",
    "# endclass DmlCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
