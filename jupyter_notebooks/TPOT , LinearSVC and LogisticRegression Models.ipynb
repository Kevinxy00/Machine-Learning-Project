{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from tpot import TPOTClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    42116\n",
       "1    18703\n",
       "0     5801\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load data\n",
    "df0=pd.read_csv(\"data/group_0.csv\")\n",
    "df1 = pd.read_csv(\"data/group_1.csv\")\n",
    "df2=pd.read_csv(\"data/group_2.csv\")\n",
    "cobn=[df0,df1,df2]\n",
    "df=pd.concat(cobn)\n",
    "# df=df. sample(n=5000, axis=0)\n",
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'count'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-e1f45424557d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'count'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49965, 16655)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split data into test & train\n",
    "X = df[\"abstract\"]\n",
    "y = df[\"label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y,train_size=0.75, test_size=0.25)\n",
    "\n",
    "# transform X and y to lists for processing\n",
    "X_train = X_train.tolist()\n",
    "X_test = X_test.tolist()\n",
    "y_train = y_train.tolist()\n",
    "y_test = y_test.tolist()\n",
    "len(X_train),len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word to vector\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', stop_words='english',ngram_range=(1, 3))\n",
    "hash_vectorizer = HashingVectorizer(analyzer='word',stop_words='english', ngram_range=(1, 3),n_features=69000)\n",
    "X_train=hash_vectorizer.fit_transform(X_train)\n",
    "X_test=hash_vectorizer.fit_transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=25.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit to logistic regression function\n",
    "classifier = LogisticRegression(C=25.0, dual=False, penalty='l1')\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.0\n",
      "Testing accuracy: 0.769198438907235\n"
     ]
    }
   ],
   "source": [
    "# training and testing data score\n",
    "print(f\"Training accuracy: {classifier.score(X_train, y_train)}\")\n",
    "print(f\"Testing accuracy: {classifier.score(X_test, y_test)}\")\n",
    "\n",
    "# Training accuracy: 0.8489742819973982\n",
    "# Testing accuracy: 0.7805463824677275"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Actual  Prediction\n",
       "0       2           2\n",
       "1       2           1\n",
       "2       1           2\n",
       "3       2           2\n",
       "4       1           1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making predictions\n",
    "predictions = classifier.predict(X_test)\n",
    "pd.DataFrame({\"Prediction\": predictions, \"Actual\": y_test}).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             precision    recall  f1-score   support\n",
      "\n",
      "Highly Prestigious Journals       0.56      0.40      0.47      1450\n",
      "      Medium Impact Journal       0.63      0.63      0.63      4676\n",
      "         Low Impact Journal       0.85      0.88      0.87     10529\n",
      "\n",
      "                avg / total       0.76      0.77      0.76     16655\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = [\"Highly Prestigious Journals\", \"Medium Impact Journal\",\"Low Impact Journal\"]\n",
    "report = classification_report(y_test, predictions, target_names=target_names)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the model\n",
    "import pickle\n",
    "# pickle.dump(classifier,open('Paper_qlty_logReg_model2', 'wb'))\n",
    "classifier.classes_\n",
    "#export the code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test: loading in the pickled model\n",
    "clf2 = pickle.load(open('Paper_qlty_logReg_model2', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cancer progression involves the gradual loss of a differentiated phenotype and acquisition of progenitor and stem-cell-like features. Here, we provide novel stemness indices for assessing the degree of oncogenic dedifferentiation. We used an innovative one-class logistic regression (OCLR) machine-learning algorithm to extract transcriptomic and epigenetic feature sets derived from non-transformed pluripotent stem cells and their differentiated progeny. Using OCLR, we were able to identify previously undiscovered biological mechanisms associated with the dedifferentiated oncogenic state. Analyses of the tumor microenvironment revealed unanticipated correlation of cancer stemness with immune checkpoint expression and infiltrating immune cells. We found that the dedifferentiated oncogenic phenotype was generally most prominent in metastatic tumors. Application of our stemness indices to single-cell data revealed patterns of intra-tumor molecular heterogeneity. Finally, the indices allowed for the identification of novel targets and possible targeted therapies aimed at tumor differentiation.Poznan Sciences; Greater Poland Cancer Center; for Molecular Oncology'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.abstract[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This abstract has high probablity of published in has NOT high probablity of published in HIGH or MEDIUM Impact Journal (Label-3) \n"
     ]
    }
   ],
   "source": [
    "verify_abst=\"this is a test\"#df0.abstract[432]\n",
    "\n",
    "Journal_class = {0:\"has high probablity of published in HIGH Impact Journals\", \n",
    "                 1:\"has high probablity of published in MEDIUM Impact Journal\",\n",
    "                 2:\"has NOT  a chance of published in HIGH or MEDIUM Impact Journal\"}\n",
    "def model_predict(s):\n",
    "    string = []\n",
    "    string.append(s)\n",
    "    test = hash_vectorizer.fit_transform(string)\n",
    "    result = clf2.predict(test)\n",
    "    return result[0]\n",
    "\n",
    "\n",
    "print(f\"This abstract has high probablity of published in {Journal_class[model_predict(verify_abst)]}\\\n",
    " (Label-{model_predict(verify_abst)+1}) \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Using AutoML to Generate Machine Learning Pipelines with TPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    42116\n",
       "1    18703\n",
       "0     5801\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load data\n",
    "df0=pd.read_csv(\"data/group_0.csv\")\n",
    "df1 = pd.read_csv(\"data/group_1.csv\")\n",
    "df2=pd.read_csv(\"data/group_2.csv\")\n",
    "cobn=[df0,df1,df2]\n",
    "df=pd.concat(cobn)\n",
    "# df=df. sample(n=5000, axis=0)\n",
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49965, 16655)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split data into test & train\n",
    "X = df[\"abstract\"]\n",
    "df.rename(columns={'label': 'class'}, inplace=True)# # rename the target/response variable as class\n",
    "y = df[\"class\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y,train_size=0.75, test_size=0.25)\n",
    "\n",
    "# transform X and y to lists for processing\n",
    "X_train = X_train.tolist()\n",
    "X_test = X_test.tolist()\n",
    "y_train = y_train.tolist()\n",
    "y_test = y_test.tolist()\n",
    "len(X_train),len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word to vector\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', stop_words='english',ngram_range=(1, 3))\n",
    "hash_vectorizer = HashingVectorizer(analyzer='word',stop_words='english', ngram_range=(1, 3),n_features=69000)\n",
    "X_train=hash_vectorizer.fit_transform(X_train)\n",
    "X_test=hash_vectorizer.fit_transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0     int64\n",
       "pmid           int64\n",
       "title         object\n",
       "abstract      object\n",
       "journal       object\n",
       "class          int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature data type convertion to array type\n",
    "training_feature=X_train.toarray()\n",
    "test_feature=X_test.toarray()\n",
    "type(test_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  51%|█████     | 61/120 [2:24:38<3:16:04, 199.40s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.7288120369611495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  76%|███████▌  | 91/120 [4:01:55<1:40:29, 207.93s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 2 - Current best internal CV score: 0.7288120369611495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              06, 243.34s/pipeline]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 5 - Current best internal CV score: 0.7288120369611495\n",
      "\n",
      "Best pipeline: LinearSVC(input_matrix, C=10.0, dual=False, loss=squared_hinge, penalty=l2, tol=0.0001)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TPOTClassifier(config_dict={'sklearn.naive_bayes.GaussianNB': {}, 'sklearn.naive_bayes.BernoulliNB': {'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0], 'fit_prior': [True, False]}, 'sklearn.naive_bayes.MultinomialNB': {'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0], 'fit_prior': [True, False]}, 'sklearn.tree.DecisionT....45,\n",
       "        0.5 ,  0.55,  0.6 ,  0.65,  0.7 ,  0.75,  0.8 ,  0.85,  0.9 ,\n",
       "        0.95,  1.  ])}}}},\n",
       "        crossover_rate=0.1, cv=5, disable_update_check=False,\n",
       "        early_stop=None, generations=5, max_eval_time_mins=5,\n",
       "        max_time_mins=None, memory=None, mutation_rate=0.9, n_jobs=1,\n",
       "        offspring_size=20, periodic_checkpoint_folder=None,\n",
       "        population_size=20, random_state=None, scoring=None, subsample=1.0,\n",
       "        verbosity=2, warm_start=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpot = TPOTClassifier(generations=5, population_size=20, verbosity=2)\n",
    "tpot.fit(training_feature, y_train)\n",
    "# print(tpot.score(validation_indices, testing_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7448\n"
     ]
    }
   ],
   "source": [
    "#TPOT accuracy\n",
    "X_test = X_test.toarray()\n",
    "y_test=np.array(y_test)\n",
    "print(tpot.score(X_test, y_test)) #=0.7448\n",
    "# type(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#export the best algorithm code\n",
    "tpot.export('tpot-journal-pipeline2.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LenearSVC model as suggested by TPOT best pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from sklearn.svm import LinearSVC ## LinearSVC is selectd based on the  optimization TPOT best pipeline result\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from tpot import TPOTClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    42116\n",
       "1    18703\n",
       "0     5801\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "df_0=pd.read_csv(\"data/group_0.csv\")\n",
    "df_1 = pd.read_csv(\"data/group_1.csv\")\n",
    "df_2=pd.read_csv(\"data/group_2.csv\")\n",
    "cobn=[df_0,df_1,df_2]\n",
    "tPOTdf=pd.concat(cobn)\n",
    "# df=df. sample(n=1000, axis=0)\n",
    "tPOTdf.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53296, 13324)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split data into test & train\n",
    "X = tPOTdf[\"abstract\"]\n",
    "y = tPOTdf[\"label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y,train_size=0.8, test_size=0.2)\n",
    "\n",
    "# transform X and y to lists for processing\n",
    "X_train2 = X_train.tolist()\n",
    "X_test2 = X_test.tolist()\n",
    "y_train2 = y_train.tolist()\n",
    "y_test2 = y_test.tolist()\n",
    "len(X_train2),len(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word to vector\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', stop_words='english',ngram_range=(1, 3))\n",
    "hash_vectorizer = HashingVectorizer(analyzer='word',stop_words='english', ngram_range=(1, 3),n_features=69000)\n",
    "feature_train=hash_vectorizer.fit_transform(X_train2)\n",
    "feature_test=hash_vectorizer.fit_transform(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=10.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3 = LinearSVC(C=10.0, dual=False, loss='squared_hinge', penalty='l2', tol=0.0001)\n",
    "clf3.fit(feature_train, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.0\n",
      "Testing accuracy: 0.7530771540078055\n"
     ]
    }
   ],
   "source": [
    "# training and testing data score\n",
    "print(f\"Training accuracy: {clf3.score(feature_train, y_train2)}\")\n",
    "print(f\"Testing accuracy: {clf3.score(feature_test, y_test2)}\")\n",
    "\n",
    "# Training accuracy: 1.0\n",
    "# Testing accuracy: 0.716"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Actual  Prediction\n",
       "0       2           2\n",
       "1       2           2\n",
       "2       1           2\n",
       "3       2           2\n",
       "4       1           1"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making predictions\n",
    "predictions = clf3.predict(feature_test)\n",
    "pd.DataFrame({\"Prediction\": predictions, \"Actual\": y_test2}).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the model\n",
    "import pickle\n",
    "# pickle.dump(clf3,open('linearSVC_model2', 'wb'))\n",
    "classifier.classes_\n",
    "#export the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test: loading in the pickled model\n",
    "clf3 = pickle.load(open('linearSVC_model', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This abstract has high probablity of published in has NOT  a chance of published in HIGH or MEDIUM Impact Journal (or it is LEVEL-3 abstract) \n"
     ]
    }
   ],
   "source": [
    "verify_abst=\"this is a test\"#df0.abstract[432]\n",
    "\n",
    "Journal_class = {0:\"has high probablity of published in HIGH Impact Journals\", \n",
    "                 1:\"has high probablity of published in MEDIUM Impact Journal\",\n",
    "                 2:\"has NOT  a chance of published in HIGH or MEDIUM Impact Journal\"}\n",
    "def model_predict(s):\n",
    "    string = []\n",
    "    string.append(s)\n",
    "    test = hash_vectorizer.fit_transform(string)\n",
    "    result = clf3.predict(test)\n",
    "    return result[0]\n",
    "\n",
    "\n",
    "print(f\"This abstract has high probablity of published in {Journal_class[model_predict(verify_abst)]}\\\n",
    " (or it is LEVEL-{model_predict(verify_abst)+1} abstract) \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
